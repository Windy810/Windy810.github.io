---
layout: post
title: "Machine_Learning_at_the _Wireless_Edge"
date: 2022-10-28
description: "概述无线网络边缘端的分布式学习的一些成果，包括两个主题：联邦学习和去中心化学习"
tags: machinelearning
category: blog
comments: true
sitemap: true
image:
  path: "https://imgur.com/xCQo3Mx.png"
---
* content
{:toc}

URL: https://www.ee.bgu.ac.il/~haimp/VincentPoorMLCOM22.pdf  
URL 2: 【【直播回放】IEEE TNSE杰出讲座系列（五） 2022年10月28日8点场】 https://www.bilibili.com/video/BV1Qv4y1S73b/?share_source=copy_web&vd_source=a55b4d059ed3cf630e3c5f3bb2f7e95b


## **引言**（机器学习与无线网络的关联）：

- 使用机器学习优化通信网络
- 在移动设备上学习

## 1. Motivation

### 1.1 机器学习最新现状

- 大量数据可用，计算能力提高
- 标准的机器学习是集中式的，可以访问所有数据
- 在云端使用软件工具运行模型，通过特殊硬件加速

![Untitled](https://imgur.com/xCQo3Mx.png)

### 1.2 无线边缘的机器学习

- 集中机器学习不适合一些新兴应用，如：自动驾驶、急救网络、医疗网络
- 这些场景特殊的原因：数据由边缘产生、有限容量上行链路 、低延迟和高可靠性、数据隐私/安全、可扩展性和局部性

以上原因促使了机器学习需要更接近网络边缘

![Untitled](https://imgur.com/2z4ZBoQ.png)

### 1.3 网络化机器学习模型

- 标准机器学习：数据存储在云端；在云端进行训练；没有隐私
- 联邦机器学习：云与用户设备同时机器学习；只有部分数据存储在云端；促进隐私
- 去中心化机器学习：没有类似于云的基础设施；数据完全分布；协作智能；隐私

![Untitled](https://imgur.com/NpXEPP8.png)

## 2. 联邦学习

### 2.1 基本架构

【数据集】终端用户(UE)使用本地的原始数据→

【训练】终端用户使用共享模型进行训练→

【联邦计算】边缘节点(AP)从终端收集权重并更新共享模型→迭代至收敛

![Untitled](https://imgur.com/aFewsqH.png)

### 2.2 需要解决的问题

- 与边缘节点的交流只能通过无线信道
- 无线介质是共享的而且有限：每轮更新只能选取部分设备；因为干扰传输不可靠

![Untitled](https://imgur.com/hPUU0zs.png)

### 2.3 调度机制

- 随机调度
- 轮询：分组再依次选组
- 比例公平：选择最强信噪比的终端用户

![Untitled](https://imgur.com/65XiqPK.png)

### 2.4 性能指标

- 更新的前提条件：
1. 终端用户被调度器选择
2. 接收的信噪比大于解码阈值
- 量化训练有效性的指标：达到解法精度所需要的沟通轮次

【注】$\epsilon$-accurate solution：最大化一个强凸函数（原始解和对偶解是相同的），此时，只有两个解，原始解和对偶解，彼此在$\epsilon$之内

![Untitled](https://imgur.com/7RbB76r.png)

【参数注解】$\epsilon$是试图实现的二元差，$\theta$是信噪比阈值，其他如图所示（具体细节可以暂时不注意）

![联邦学习的收敛速度](https://imgur.com/15GhQeO.png)

联邦学习的收敛速度

【实验说明】

- 左图为解码阈值很大，意味着传输通道很差，所以需要高信噪比才能成功解码，此种情况比例公平的方法表现最好
- 右图解码阈值很小，说明传输通道很不错，较低的信噪比也能解码，此时轮询表现最好

原因说明：当信道很好的时候，意味着数据包都通过了，所以每个数据包等量地被使用是最合适的

![Untitled](https://imgur.com/bfmGGcD.png)

【结论】调度协议十分重要

在较优的频道中测试，以极低的信噪比，只比较随机调度和轮询，左边为损失，右图为精准度，基于MNIST手写数据集建立SVM模型

- 轮询在两方面表现都明显较好
- 如果交流回合够多，则两者差不多，次数较少的情况下，轮询收敛快很多

![Untitled](https://imgur.com/CSFru05.png)

### 2.5 优化调度

- 设计想法：因为在类似于传感器网络中，数据的及时性和准确性同样重要，所以要确保诸如此类情况，终端设备的数据在一个合理的时间限制内上传
- 设计指标：信息时代/信息的年龄(AoI)
- 设计思路：每回交流，如果被选择，则将AoI降为0，否则，加1

![Untitled](https://imgur.com/X5SmANb.png)

- **最终算法**：于是，据此设计一个使得平均信息年龄最小化的算法，称为**无线轮询**

【实验分析】如上类似的实验条件，比较无线轮询和最大包算法（每一轮发出尽可能最多的包）的表现，发现其明显优于该算法

![Untitled](https://imgur.com/Dw35124.png)

### 2.6 联邦学习中的隐私保护

- 最初因为数据保留在终端上，认为其可以保护隐私
- 后来发现用户数据可以从模型参数中推断出来
- 所以用户数据的隐私是联邦学习的一个问题

![Untitled](https://imgur.com/la90Qpa.png)

所以提出了一些解决方案，类似于**差分隐私**，特征如下

- 分为两个数据集，一个有私人信息，一个没有私人信息，其他部分相同
- 不能通过统计查询（高概率）区分
- 有时候可以向数据添加噪声来实现差异隐私，但是会影响性能
- 需要在隐私和性能之间进行权衡

![Untitled](https://imgur.com/5lvYZKk.png)

【实验说明】对MNIST数据集进行卷积神经网络拟合

- 紫色线为没有添加任何噪音
- $\epsilon$参数越小，则差分隐私的隐私程度越大，其性能也就越差

![Untitled](https://imgur.com/qYVxuuN.png)

## 3. 去中心化学习

**特性**：模型在终端用户设备上构建，但是所有的模型共享和协作都是以点对点的方式进行的

### 3.1 基本模型

【图分析】一个二分图，一部分为训练样本，另一部分为学习体，图片表示了学习体可以访问的训练数据库。可以看出，并不是每个学习体都可以访问每个训练数据库

集中学习：学习体数量=1

去中心化学习：学习体数量=训练样本数

![Untitled](https://imgur.com/v3zHoWC.png)

但是本地的学习是**局部不连贯**的：

如果两个学习体可访问的训练数据库有重合部分，例如$L_1$和$L_m$都可以访问$x_1,y_1$，但是他们得到的$\hat{f}_1$和$\hat{f}_m$是不一样的，这就是局部不连贯

如果一个模型中有局部不连贯，那么其是次优的

![Untitled](https://imgur.com/fM4Bs5c.png)

### 3.2 协同算法

所以为了解决上述问题，提出了一个**协同算法**

【算法阐述】在学习体1完成训练后，将其对目标的预估写回原数据库，再进入学习体2的训练，这样，学习体1的经验即可被转移到学习体2。依次迭代，反复这样做以后，结果则会连贯地收敛到全局解

【引申结论】分布式机器学习可以通过消除此类不一致来获得接近最优的算法

![Untitled](https://imgur.com/9gkJQl6.png)

【实验分析】有50个传感器构成的学习体和50个训练数据库，假设回归函数f是线性的，i和j称为邻居，其距离定义为r，代表连通性，r越大则二分图中的边越多，即每个学习体可以访问的数据库越多。使用线性内核进行内核学习，纵轴代表平均平方误差

- 图的最底部有一条直线，是集中学习的表现
- 较高的虚线为没有协同算法的，需要连通性很大的情况下才能达到集中学习的表现
- 较矮的虚线为有协同算法的，明显在合理的时间内和集中学习的表现一样好

![Untitled](https://imgur.com/9AEThHd.png)

### 3.3 分布式强化学习

- 强化学习：用于解决马尔科夫决策问题，即用于寻找最优决策和控制策略
- Q-learning：特殊类型的强化学习，每次学习迭代中，都需要更新的Q-function；利用梯度下降等方法来收敛到最优
- 分布式强化学习：因为无法在本地计算梯度了，所以可以根据
    1. 本地邻居的数据来计算局部梯度
    2. 在邻居间寻找共识，达成一致
    3. 在连通性上收敛到全局最优

![Untitled](https://imgur.com/aYpzOxc.png)

【模拟】Q函数有四个值，将其随机扔到一个区域里，然后使用两个最近的邻居来连通，再来绘制图。左图为一个典型的agent，右图为十个agent

- 左图能达到集中，汇聚在一起了
- 右图显示，一开始他们变化很大，但是由于算法的一致性，最后他们会收敛到一起，达成共识

![Untitled](https://imgur.com/SXmqNTB.png)

## 4. Conclusion

无线网络可以成为机器学习的平台

- 联邦学习：边缘设备与终端用户设备交互来学习常用模型
- 去中心化学习：终端用户设备与另一台设备交互来协作学习模型或动作

![Untitled](https://imgur.com/tzhE5tQ.png)

【一些研究上的问题】

- 设备限制
- 通信效率
- 边缘数据有限
- 安全和隐私

![Untitled](https://imgur.com/xwu3GLq.png)

【相关论文】

![Untitled](https://imgur.com/QNgl4ym.png)

## 5. 提问与回答

- 除了可能影响性能的差分隐私，如何解决联邦学习中的隐私问题？
    
    A：①使用某种同态加密可以实现在不解密的情况下进行聚合（对聚合器进行保密）②使用区块链分布式账本，由终端用户自行进行聚合。主要取决于想对谁进行保密，防范谁。
    
- 两个运行在不同服务器的不同的联邦学习应用有个公共池/重叠池？
    
    A：聚合器要做的工作是区分不同的子集，为其提供相应的平等服务。但是划分不同的进程比较难以做到。
    
    不能一味地进行聚合，比如有些终端是视频数据，有些是图片数据，两者聚合并没有好处，应当让其以自适应的方式进行集群聚合
    
- 关于去中心化学习
    
    可以将分布式学习变化为多层级的，不一定只是看起来的点对点。例如边缘计算加入雾计算/一些控制器等等方法